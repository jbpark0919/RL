{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "150881b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:56:52.169872Z",
     "start_time": "2025-04-21T11:56:52.156480Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from utils import gen_wrapped_env\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb3475c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:56:52.504262Z",
     "start_time": "2025-04-21T11:56:52.484104Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, actions, agent_indicator=10):\n",
    "        self.actions = actions\n",
    "        self.agent_indicator = agent_indicator\n",
    "        self.alpha = 0.01\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.2\n",
    "        self.q_values = defaultdict(lambda: [0.0] * actions)\n",
    "\n",
    "    def _convert_state(self, s):\n",
    "        return np.where(s == self.agent_indicator)[0][0]\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        state = self._convert_state(state)\n",
    "        next_state = self._convert_state(next_state)\n",
    "\n",
    "        q_value = self.q_values[state][action]\n",
    "\n",
    "        ########################## Write Code ####################################\n",
    "        next_q_value = max(self.q_values[next_state])\n",
    "        td_error = reward + self.gamma * next_q_value - q_value\n",
    "        self.q_values[state][action] = q_value + self.alpha * td_error\n",
    "\n",
    "        ###########################################################################\n",
    "    def act(self, state):\n",
    "        #################### Write epsilion greedy code ###########################\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            state = self._convert_state(state)\n",
    "            q_values = self.q_values[state]\n",
    "            action = int(np.argmax(q_values))\n",
    "        ###########################################################################\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0feb884b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:56:52.635772Z",
     "start_time": "2025-04-21T11:56:52.618642Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_q_learning_with_gamma(gamma, episodes):\n",
    "    env = gen_wrapped_env('MiniGrid-Empty-6x6-v0')\n",
    "    obs = env.reset()\n",
    "    agent_position = obs[0]\n",
    "\n",
    "    q_agent = QLearning(actions=4, agent_indicator=agent_position)\n",
    "    q_agent.gamma = gamma\n",
    "\n",
    "    ep_rewards = []\n",
    "    q_values_over_time = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        q_action = q_agent.act(obs)\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            next_obs, reward, done, info = env.step(q_action)\n",
    "            next_action = q_agent.act(next_obs)\n",
    "            q_agent.update(obs, q_action, reward, next_obs, next_action)\n",
    "\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "            q_action = next_action\n",
    "\n",
    "        ep_rewards.append(total_reward)\n",
    "\n",
    "        if (ep + 1) % 100 == 0:\n",
    "            all_qs = [q for qs in q_agent.q_values.values() for q in qs]\n",
    "            avg_q = np.mean(all_qs)\n",
    "            q_values_over_time.append(avg_q)\n",
    "\n",
    "    env.close()\n",
    "    return ep_rewards, q_agent.q_values, q_values_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4594c502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:56:52.782724Z",
     "start_time": "2025-04-21T11:56:52.776949Z"
    }
   },
   "outputs": [],
   "source": [
    "gammas = [0.1, 0.5, 0.9, 0.99]\n",
    "reward_logs = {}\n",
    "q_tables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee6bf240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:56:52.936706Z",
     "start_time": "2025-04-21T11:56:52.929160Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./logs/gamma_rewards\", exist_ok=True)\n",
    "os.makedirs(\"./logs/q_tables\", exist_ok=True)\n",
    "os.makedirs(\"./logs/plots\", exist_ok=True)\n",
    "os.makedirs(\"./logs/plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fc2ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-21T11:56:51.951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-learning with gamma = 0.1\n"
     ]
    }
   ],
   "source": [
    "for g in gammas:\n",
    "    print(f\"Training Q-learning with gamma = {g}\")\n",
    "    rewards, q_vals, q_values_over_time = run_q_learning_with_gamma(gamma=g, episodes=100000)\n",
    "\n",
    "    reward_logs[g] = rewards\n",
    "    q_tables[g] = q_vals\n",
    "\n",
    "    # 1. Reward 로그 저장\n",
    "    df = pd.DataFrame({\"Episode\": range(len(rewards)), \"Reward\": rewards})\n",
    "    df.to_csv(f\"./logs/gamma_rewards/reward_gamma_{g}.csv\", index=False)\n",
    "\n",
    "    # 2. Q-table 저장\n",
    "    q_dict = {str(int(s)): np.round(q, 5).tolist() for s, q in q_vals.items()}\n",
    "    with open(f\"./logs/q_tables/q_table_gamma_{g}.json\", 'w') as f:\n",
    "        json.dump(q_dict, f, indent=2)\n",
    "\n",
    "    # 3. Q-value 수렴 시각화 및 저장\n",
    "    episodes_recorded = list(range(100, len(q_values_over_time)*100 + 1, 100))\n",
    "    plt.figure()\n",
    "    plt.plot(episodes_recorded, q_values_over_time)\n",
    "    plt.title(f\"Q-value Convergence (Gamma = {g})\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Avg Q-value\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"./logs/plots/q_value_convergence_gamma_{g}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81924aa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-21T11:56:52.309Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Episode Reward 그래프\n",
    "plt.figure(figsize=(12, 6))\n",
    "for g in gammas:\n",
    "    plt.plot(pd.Series(reward_logs[g]).rolling(10000).mean(), label=f\"γ={g}\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Episode Reward (Moving Avg)\")\n",
    "plt.title(\"Q-learning Episode Reward vs Gamma\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./logs/plots/episode_reward_vs_gamma.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f94ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-21T11:56:55.654Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_avg_qvalue_per_action(gammas, q_table_dir=\"./logs/q_tables\", save_path=\"./logs/plots/avg_qvalue_per_action_by_gamma.png\"):\n",
    "    action_count = 4  # 행동 개수\n",
    "    gamma_labels = []\n",
    "    avg_qvalues_by_action = []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        with open(f\"{q_table_dir}/q_table_gamma_{gamma}.json\", 'r') as f:\n",
    "            q_dict = json.load(f)\n",
    "\n",
    "        q_values = list(q_dict.values())\n",
    "        q_array = np.array(q_values)  # shape: (num_states, 4)\n",
    "\n",
    "        if q_array.shape[0] == 0:\n",
    "            continue  # 혹시라도 비어있을 경우 스킵\n",
    "\n",
    "        avg_q_per_action = np.mean(q_array, axis=0)  # shape: (4,)\n",
    "        avg_qvalues_by_action.append(avg_q_per_action)\n",
    "        gamma_labels.append(str(gamma))\n",
    "\n",
    "    avg_qvalues_by_action = np.array(avg_qvalues_by_action).T  # shape: (4, num_gammas)\n",
    "\n",
    "    x = np.arange(len(gamma_labels))\n",
    "    bar_width = 0.2\n",
    "    action_labels = [\"Left\", \"Right\", \"Forward\", \"Toggle\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(action_count):\n",
    "        plt.bar(x + i * bar_width, avg_qvalues_by_action[i], width=bar_width, label=action_labels[i])\n",
    "\n",
    "    plt.xticks(x + bar_width * (action_count - 1) / 2, gamma_labels)\n",
    "    plt.xlabel(\"Gamma (γ)\")\n",
    "    plt.ylabel(\"Average Q-value\")\n",
    "    plt.title(\"Average Q-value per Action by Gamma\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "plot_avg_qvalue_per_action(gammas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
